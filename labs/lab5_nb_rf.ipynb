{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Naive Bayes and Random Forests\n",
    "In this lab we'll take a closer look the [Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) and [Random Forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangyu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Naive Bayes is famously effective for spam email classification. We'll try our hand at applying NB on the [Spam email dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from the UCI data repository. Download the dataset and move it to your lab data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'data/SMSSpamCollection'\n",
    "df = pd.read_table(fname, names=['class', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "Given collection of documents $d^{(1)}, d^{(2)}, ..., d^{(N)}$ with corresponding labels $y^{(i)}$, we first need to extract features from each document. We'll do this by computing word counts of each word in each document. This is commonly referred to as a [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation.\n",
    "\n",
    "After doing some preprocessing of our data, we'll have: $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, where:\n",
    "* $x^{(i)} \\in R^s$ is a word count feature vector for document $i$, and $s$ is the size of the vocabulary. For the purposes of this lab, we define our vocabulary to be the set of words that appear in our dataset of documents.\n",
    "$$ x^{(i)} = (\\theta^{(i)}_1, \\theta^{(i)}_2, ..., \\theta^{(i)}_s)$$\n",
    "$$\\theta^{(i)}_k = \\text{ the number of times word $k$ appears in document $i$}$$\n",
    "\n",
    "\n",
    "* $y^{(i)} \\in \\{0, 1\\}$ is its class. In the spam email classification task, 0 denotes spam, 1 denotes ham.\n",
    "\n",
    "Before we can use sklearn's Naive Bayes classifier, we need to compute word count feature vectors. We can do this manually like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def get_all_words(docs):\n",
    "    '''\n",
    "    docs: pandas.Series of strings\n",
    "    Returns: set of words(strings)\n",
    "    '''\n",
    "    # Compute all the words in the docs\n",
    "    words = set()\n",
    "    for d in docs:\n",
    "        # this will strip punctuation\n",
    "        # ref: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        d = d.translate(dict.fromkeys(string.punctuation))\n",
    "        words = words.union(set(d.split()))\n",
    "    return words\n",
    "\n",
    "def make_word_cnt_feats(docs, vocabulary=None):\n",
    "    '''\n",
    "    docs: iterable(ie: list or pandas.Series) of strings\n",
    "    vocabulary: set of words(strings) that appear in docs\n",
    "    Returns: numpy matrix of the word count features\n",
    "    '''\n",
    "    if vocabulary is None:\n",
    "        vocabulary = get_all_words(docs)\n",
    "    words = sorted(vocabulary)\n",
    "    word_dict = {w: idx for idx, w in enumerate(words)}\n",
    "    \n",
    "    # feature matrix will be of size n x (size of vocabulary + 1)\n",
    "    # add 1 to account for words that dont appear in the vocabulary to avoid random issues\n",
    "    vocab_size = len(words)\n",
    "    word_cnt_features = np.zeros((len(docs), len(words) + 1))\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        doc = docs[idx]\n",
    "        doc = doc.translate(dict.fromkeys(string.punctuation))\n",
    "        words = doc.split()\n",
    "        for w in words:\n",
    "            word_idx = word_dict.get(w, vocab_size)\n",
    "            word_cnt_features[idx, word_idx] += 1\n",
    "            \n",
    "    return word_cnt_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (5572, 15692)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = get_all_words(df['text'])\n",
    "feature_matrix = make_word_cnt_feats(df['text'], vocabulary)\n",
    "print('Feature matrix shape: {}'.format(feature_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that sklearn actually has a built-in method to do this kind of word count featurization! (But of course, it's good to get your hands dirty with manual featurization from time to time to understand all the ingredients of these algorithms). See sklearn's [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) function for more details.                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8713 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 74169 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "word_cnt_feats = count_vectorizer.fit_transform(df['text'])\n",
    "word_cnt_feats # this is a sparse matrix\n",
    "# looks like this gives us fewer columns, which means we didn't prune\n",
    "# a bunch of unwanted terms(maybe words with numbers?) in our code above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) has a bunch of useful functions that you should checkout. The important ones that we'll use are:\n",
    "* [fit](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit): this learns the vocabulary of the input collection of strings/\"documents\"\n",
    "* [transform](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform): given a collection of documents, this produces the word count features for the documents\n",
    "* fit_transform: calls fit, then transform\n",
    "\n",
    "Note that the word count feature matrix(aka document-term matrix) is in sparse format. This is not an issue for the sklearn models. But if you want to expand them to non-sparse numpy arrays, use the toarray function. Note that this can be problematic for bigger datasets! We can somewhat get around this b/c this dataset is small enough that we can fully expand a 5k by 8k matrix(since it's sparse enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = word_cnt_feats.toarray()\n",
    "Y = (df['class'] == 'ham')\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data in the document-term matrix format, we can use the Naive Bayes classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "nb = MultinomialNB(alpha=1)\n",
    "nb.fit(X_train, Y_train)\n",
    "preds = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.975\n",
      "F1 Score: 0.985\n",
      "ROC AUC Score: 0.943\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.3f}'.format(accuracy_score(preds, Y_test)))\n",
    "print('F1 Score: {:.3f}'.format(f1_score(preds, Y_test)))\n",
    "print('ROC AUC Score: {:.3f}'.format(roc_auc_score(preds, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Tasks\n",
    "Now it's your turn to use it on the Naive Bayes model. Pick an evaluation metric to use for cross validation like precision, recall, F1 score, ROC AUC score, etc. In the spam classification setting, do you think it makes sense to optimize for higher precision? recall? f1 score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) Pick an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Note that the code above fixed alpha to 1. But as we've learned in class and last week's lab, we should use cross validation for any kind of parameter search. Use [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) to do the cross validation over 5 folds to find the alpha that works best on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cv_multinom_bayes(x_train, y_train, k_folds_num, alpha_list):\n",
    "    y_train_np = y_train.values\n",
    "    kf = KFold(n_splits = k_folds_num)\n",
    "    # dictionary of results:\n",
    "    results_nb = {}\n",
    "    # split data into k-folds, and loop over each fold\n",
    "    for train_idx, test_idx in kf.split(x_train):\n",
    "        # split into k-folds\n",
    "        x_split_train, x_split_test = x_train[train_idx], x_train[test_idx]\n",
    "        y_split_train, y_split_test = y_train_np[train_idx], y_train_np[test_idx]\n",
    "        #loop over different alpha values\n",
    "        for alpha_value in alpha_list:\n",
    "            # intialize classifier and fit it to the split training set\n",
    "            nb_clf = MultinomialNB(alpha = alpha_value)\n",
    "            try:\n",
    "                nb_clf.fit(x_split_train, y_split_train)\n",
    "            except:\n",
    "                pdb.set_trace()\n",
    "                \n",
    "            # make predictions\n",
    "            y_pred = nb_clf.predict(x_split_test)\n",
    "            # intialize the main dictionary key\n",
    "            model_key = (alpha_value)\n",
    "            results_nb[model_key] = {}\n",
    "            # write evaluation results to dictionary\n",
    "            results_nb[model_key]['Accuracy']= results_nb[model_key].get('Accuracy', 0) + accuracy_score(y_pred, y_split_test)/k_folds_num\n",
    "            results_nb[model_key]['Precision']= results_nb[model_key].get('Precision', 0) + precision_score(y_pred, y_split_test)/k_folds_num\n",
    "            results_nb[model_key]['Recall']= results_nb[model_key].get('Recall', 0) + recall_score(y_pred, y_split_test)/k_folds_num\n",
    "            results_nb[model_key]['F1']= results_nb[model_key].get('F1', 0) + f1_score(y_pred, y_split_test)/k_folds_num\n",
    "   #print results\n",
    "    for model, model_perf in results_nb.items():\n",
    "        print(\"Model parameters: {}\".format(model, model_perf))\n",
    "        for eva_metric, eva_value in model_perf.items():\n",
    "            print(\"{}: {}\".format(eva_metric, eva_value))\n",
    "        print(\"————————————————————————————————\")\n",
    "    #return the results, in case I want to do anything with it.\n",
    "    return results_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 1\n",
      "Accuracy: 0.1970059880239521\n",
      "Precision: 0.19825783972125435\n",
      "Recall: 0.19825783972125435\n",
      "F1: 0.19825783972125435\n",
      "————————————————————————————————\n",
      "Model parameters: 5\n",
      "Accuracy: 0.19221556886227545\n",
      "Precision: 0.2\n",
      "Recall: 0.19133333333333333\n",
      "F1: 0.19557069846678027\n",
      "————————————————————————————————\n",
      "Model parameters: 10\n",
      "Accuracy: 0.18802395209580838\n",
      "Precision: 0.2\n",
      "Recall: 0.18697068403908795\n",
      "F1: 0.19326599326599325\n",
      "————————————————————————————————\n",
      "Model parameters: 25\n",
      "Accuracy: 0.181437125748503\n",
      "Precision: 0.2\n",
      "Recall: 0.18050314465408807\n",
      "F1: 0.1897520661157025\n",
      "————————————————————————————————\n",
      "Model parameters: 100\n",
      "Accuracy: 0.17305389221556886\n",
      "Precision: 0.2\n",
      "Recall: 0.17289156626506025\n",
      "F1: 0.18546042003231017\n",
      "————————————————————————————————\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'Accuracy': 0.1970059880239521,\n",
       "  'F1': 0.19825783972125435,\n",
       "  'Precision': 0.19825783972125435,\n",
       "  'Recall': 0.19825783972125435},\n",
       " 5: {'Accuracy': 0.19221556886227545,\n",
       "  'F1': 0.19557069846678027,\n",
       "  'Precision': 0.2,\n",
       "  'Recall': 0.19133333333333333},\n",
       " 10: {'Accuracy': 0.18802395209580838,\n",
       "  'F1': 0.19326599326599325,\n",
       "  'Precision': 0.2,\n",
       "  'Recall': 0.18697068403908795},\n",
       " 25: {'Accuracy': 0.181437125748503,\n",
       "  'F1': 0.1897520661157025,\n",
       "  'Precision': 0.2,\n",
       "  'Recall': 0.18050314465408807},\n",
       " 100: {'Accuracy': 0.17305389221556886,\n",
       "  'F1': 0.18546042003231017,\n",
       "  'Precision': 0.2,\n",
       "  'Recall': 0.17289156626506025}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = [1, 5, 10, 25, 100] # you should vary these \n",
    "\n",
    "cv_multinom_bayes(X_train, Y_train, 5, alpha_list= alphas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Now that you've found the optimal alpha using cross validation over the training set, train a Naive Bayes model using the optimal alpha over the entirety of the training data and evaluate this model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Questions:\n",
    "* Naive Bayes has a seemingly stringent independence assumption. Do you think the variables in this email classification problem(word count vectors) are really independent? Does this matter? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "Two weeks ago, we played around with decision trees. In general, practictioners almost never use decision trees over Random Forests. Recall from lecture, that Random Forests takes some number of subsamples of the data:\n",
    "$S_1, S_2, ..., S_k \\subseteq \\{(x^{(i)}, y^{(i)}\\}_{i=0}^N$. On each subsample of the data, $S_i$, the Random Forests algorithm trains a decision tree on $S_i$.\n",
    "\n",
    "At classification time, when we get a datapoint $x$, the Random Forests classifier runs $x$ through decision trees $D_1, D_2, ..., D_k$, which produces a \"probability\" estimate of each class:\n",
    "\n",
    "$$P(y = 1 | x) = \\frac{\\text{number of decision trees that classified $x$ as 1}}{k}$$\n",
    "\n",
    "$$P(y = 0 | x) = \\frac{\\text{number of decision trees that classified $x$ as 1}}{k}$$\n",
    "\n",
    "Then from there, we get a class prediction by picking $y = 1$ if $P(y = 1 | x) > T$, for some threshold probability $T$.\n",
    "\n",
    "\n",
    "Instead of using the spam dataset to try out Random Forests, we'll re-use the credit loan data from Assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"credit-data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do whatever data cleaning, data imputing, etc you need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_nulls(df):\n",
    "\treturn df.isna().sum().sort_values(ascending=False)\n",
    "\n",
    "def fill_whole_df_with_mean(df):\n",
    "    num_cols = len(df.columns)\n",
    "    for i in range(0, num_cols):\n",
    "        df.iloc[:,i] = fill_col_with_mean(df.iloc[:,i])\n",
    "    return\n",
    "\n",
    "def fill_col_with_mean(df):\n",
    "\treturn df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features = ['MonthlyIncome', 'DebtRatio', 'age', 'NumberOfTimes90DaysLate','NumberOfOpenCreditLinesAndLoans'] # Pick the features you want\n",
    "df_features = df[features]\n",
    "df_target = df['SeriousDlqin2yrs']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_features, df_target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangyu/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "fill_whole_df_with_mean(X_train)\n",
    "fill_whole_df_with_mean(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "def scale_a_df(df, features_list):\n",
    "    temp_scaled = scale(df[features_list])\n",
    "    #return a DF\n",
    "    return pd.DataFrame(temp_scaled, columns= df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scale_a_df(X_train, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = scale_a_df(X_test, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Parameters\n",
    "\n",
    "The Random Forest parameters are more or less the same as those for Decision Trees. The only additional parameter is n_estimators, which is the number of decision trees to grow for the algorithm.\n",
    "* n_estimators : integer, the number of trees in the forest.\n",
    "* criterion : string,  “gini” or “entropy” \n",
    "* max_features : int, float, string or None, the number of features to consider when looking for the best split\n",
    "* max_depth : integer, the maximum depth of the tree\n",
    "* min_samples_split : int, float, the minimum number of samples required to split an internal node:\n",
    "* min_samples_leaf : int, float, optional (default=1), the minimum number of samples required to be at a leaf node:\n",
    "* min_weight_fraction_leaf : float, optional \n",
    "* max_leaf_nodes : int or None, optional (default=None)\n",
    "\n",
    "0) What do you think will be the effect of increasing the n\\_estimators parameter? If n_estimators = 1, is this any different to using a decision tree classifier(assuming you set all the other parameters like min_sample_split, max_depth, etc the same way in both cases)?\n",
    "\n",
    "1) Pick an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Pick a few values for each of the random forest parameters for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_features: the max number of features each decision tree is allowed to try.\n",
    "# increasing max_features generally improves performance, but decreases the speed of the algorithm\n",
    "max_feats = [\"auto\", \"sqrt\", \"log2\", 0.2]\n",
    "\n",
    "# n_estimators: the number of decision trees you're going to build.\n",
    "# higher the better, but of course the higher it is the slower the model is going to be.\n",
    "num_dtrees = [5, 10, 20]\n",
    "\n",
    "# max_depth: maximum depth of decision trees\n",
    "depth_list = [3,5,8]\n",
    "\n",
    "# criterion: gini or entropy\n",
    "criterion_list = [\"gini\", \"entropy\"]\n",
    "\n",
    "# min_samples_split: the minimum number of samples required to split an internal node\n",
    "min_split_list = [2,3,5]\n",
    "\n",
    "# min_samples_leaf: the minimum number of samples required to be at a leaf node:\n",
    "min_leaf_list = [1,2,3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last weeks lab, we saw how to grid search over all possible model parameters and cross validation folds. To refresh your memory, below is a slightly updated version of that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of a manual grid search for LogisticRegression on random data\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "def sample_manual_grid_search(n_samples, n_feats, splits):\n",
    "    # make the fake data\n",
    "    X = np.random.randint(0, 10, (n_samples, n_feats))\n",
    "    Y = (np.random.random(n_samples) > 0.5) # random true/false\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)\n",
    "    \n",
    "    # params to search over\n",
    "    penalties = ['l1', 'l2']\n",
    "    c_vals = [10**i for i in range(-2, 3)]\n",
    "\n",
    "    kf = KFold(n_splits=splits)\n",
    "    rows = [] # store dicts of the cross validation results for each fold\n",
    "    for p in penalties:\n",
    "        for c in c_vals:\n",
    "\n",
    "            fold_dict = {\n",
    "                    # NOTE: these key values must be exactly as they're spelled in\n",
    "                    # the sklearn model!\n",
    "                    'penalty': p,\n",
    "                    'C': c,\n",
    "            }\n",
    "            fold_results = {}\n",
    "            for fold_num, (train_idx, test_idx) in enumerate(kf.split(X_train, y_train)):\n",
    "                x_split_train, x_split_test = X_train[train_idx], X_train[test_idx]\n",
    "                y_split_train, y_split_test = y_train[train_idx], y_train[test_idx]\n",
    "                    \n",
    "                model = LogisticRegression(**fold_dict)\n",
    "                model.fit(x_split_train, y_split_train)\n",
    "                pred_probs = model.predict_proba(x_split_test)\n",
    "                loss = roc_auc_score(y_split_test, pred_probs[:, 0])\n",
    "                \n",
    "                # Keep track of the performance of the model on each fold and store the result\n",
    "                fold_results['fold_{}'.format(fold_num+1)] = loss\n",
    "                    \n",
    "            # now that we have all the fold results, update the dict containing the model params\n",
    "            # with the performance results\n",
    "            fold_dict.update(fold_results)\n",
    "            fold_dict['avg_score'] = np.mean(fold_results.values())\n",
    "            rows.append(fold_dict)\n",
    "\n",
    "    # we can create a dataframe from a list of dicts. This produces a dataframe whose columns are the keys of\n",
    "    # the dicts in the list. Recall that rows is a list of dicts\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Results for sample data with Logistic Regression:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>fold_1</th>\n",
       "      <th>fold_2</th>\n",
       "      <th>fold_3</th>\n",
       "      <th>fold_4</th>\n",
       "      <th>fold_5</th>\n",
       "      <th>penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.635608</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.633016</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.606931</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.00</td>\n",
       "      <td>0.550582</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C  avg_score    fold_1    fold_2    fold_3    fold_4    fold_5 penalty\n",
       "0    0.01   0.500000  0.500000  0.500000  0.500000  0.500000  0.500000      l1\n",
       "1    0.10   0.635608  0.314286  0.740741  0.694444  0.657143  0.771429      l1\n",
       "2    1.00   0.633016  0.514286  0.666667  0.555556  0.685714  0.742857      l1\n",
       "3   10.00   0.606931  0.600000  0.592593  0.527778  0.542857  0.771429      l1\n",
       "4  100.00   0.550582  0.571429  0.481481  0.500000  0.542857  0.657143      l1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = sample_manual_grid_search(n_samples=100, n_feats=20, splits=5)\n",
    "print(\"Grid Search Results for sample data with Logistic Regression:\")\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Do the same kind of grid search cross validation for Random Forests classifier. Which set of parameters gets the best average score over all folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predictions(clf, threshold = 0.7, x_test = X_test):\n",
    "    # threshold = the probability threshold for something to be a 0.\n",
    "    # generate array with predicted probabilities\n",
    "    pred_array = clf.predict_proba(x_test)\n",
    "\n",
    "    # initialize an empty array for the predictions\n",
    "    pred_generated = np.array([])\n",
    "\n",
    "    # predict the first entry\n",
    "    if pred_array[0][0] >= threshold:\n",
    "        pred_generated = np.hstack([pred_generated, 0])\n",
    "    else:\n",
    "        pred_generated = np.hstack([pred_generated, 1])\n",
    "\n",
    "    # loops over the rest of the array\n",
    "    for i in range(1,len(x_test)):\n",
    "        if pred_array[i][0] >= threshold:\n",
    "            pred_generated = np.vstack([pred_generated, 0])\n",
    "        else:\n",
    "            pred_generated = np.vstack([pred_generated, 1])\n",
    "\n",
    "    # return an np.array\n",
    "    return  pred_generated\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def grid_cv_RF(x_train, y_train, max_feats, num_dtrees_list, depth_list, criterion_list, k_folds_num):\n",
    "    kf = KFold(n_splits = k_folds_num)\n",
    "    # dictionary of results:\n",
    "    results_rf = {}\n",
    "    # split data into k-folds, and loop over each fold\n",
    "    for train_idx, test_idx in kf.split(x_train):\n",
    "        # split into k-folds\n",
    "        x_split_train, x_split_test = x_train.iloc[train_idx], x_train.iloc[test_idx]\n",
    "        y_split_train, y_split_test = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
    "        # loop over max_features\n",
    "        for feature_number in max_feats:\n",
    "            # loop over n_estimators\n",
    "            for num_trees in num_dtrees_list:\n",
    "                #loop over max_depth\n",
    "                for depth_num in depth_list:\n",
    "                    #loop over information gain criterion\n",
    "                    for criterion_choice in criterion_list:\n",
    "                        rf_clf = RandomForestClassifier(max_features = feature_number, n_estimators = num_trees, max_depth = depth_num, criterion = criterion_choice)\n",
    "                        rf_clf.fit(x_split_train, y_split_train)\n",
    "                        # generate prediction using threshold of 0.3 for labels 0\n",
    "                        y_pred = custom_predictions(rf_clf, threshold = 0.7, x_test = x_split_test)\n",
    "                        # intialize the main dictionary key\n",
    "                        model_key = (feature_number, num_trees, depth_num, criterion_choice)\n",
    "                        # write resuls to dictionary\n",
    "                        results_rf[model_key] = {}\n",
    "                        # write evaluation results to dictionary\n",
    "                        results_rf[model_key]['Accuracy']= results_rf[model_key].get('Accuracy', 0) + accuracy_score(y_pred, y_split_test)/k_folds_num\n",
    "                        results_rf[model_key]['Precision']= results_rf[model_key].get('Precision', 0) + precision_score(y_pred, y_split_test)/k_folds_num\n",
    "                        results_rf[model_key]['Recall']= results_rf[model_key].get('Recall', 0) + recall_score(y_pred, y_split_test)/k_folds_num\n",
    "                        results_rf[model_key]['F1']= results_rf[model_key].get('F1', 0) + f1_score(y_pred, y_split_test)/k_folds_num\n",
    "    #print results\n",
    "    for model, model_perf in results_rf.items():\n",
    "        print(\"Model parameters: {}\".format(model, model_perf))\n",
    "        for eva_metric, eva_value in model_perf.items():\n",
    "            print(\"{}: {}\".format(eva_metric, eva_value))\n",
    "        print(\"————————————————————————————————\")\n",
    "    #return the results, in case I want to do anything with it.\n",
    "    return results_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.171613e-01</td>\n",
       "      <td>-0.248654</td>\n",
       "      <td>-1.536492</td>\n",
       "      <td>-0.079579</td>\n",
       "      <td>-0.842833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.380547e-16</td>\n",
       "      <td>10.752528</td>\n",
       "      <td>-0.182472</td>\n",
       "      <td>-0.079579</td>\n",
       "      <td>2.028727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.168133e-01</td>\n",
       "      <td>-0.248409</td>\n",
       "      <td>2.254764</td>\n",
       "      <td>-0.079579</td>\n",
       "      <td>0.305791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.665707e-01</td>\n",
       "      <td>-0.248523</td>\n",
       "      <td>0.765342</td>\n",
       "      <td>-0.079579</td>\n",
       "      <td>0.114354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.380547e-16</td>\n",
       "      <td>2.656153</td>\n",
       "      <td>0.426837</td>\n",
       "      <td>-0.079579</td>\n",
       "      <td>0.497228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MonthlyIncome  DebtRatio       age  NumberOfTimes90DaysLate  \\\n",
       "0  -2.171613e-01  -0.248654 -1.536492                -0.079579   \n",
       "1   1.380547e-16  10.752528 -0.182472                -0.079579   \n",
       "2  -3.168133e-01  -0.248409  2.254764                -0.079579   \n",
       "3   1.665707e-01  -0.248523  0.765342                -0.079579   \n",
       "4   1.380547e-16   2.656153  0.426837                -0.079579   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  \n",
       "0                        -0.842833  \n",
       "1                         2.028727  \n",
       "2                         0.305791  \n",
       "3                         0.114354  \n",
       "4                         0.497228  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# important random forest parameters:\n",
    "\n",
    "# max_features: the max number of features each decision tree is allowed to try.\n",
    "# increasing max_features generally improves performance, but decreases the speed of the algorithm\n",
    "max_feats = [\"auto\", \"sqrt\", \"log2\", 0.2]\n",
    "\n",
    "# n_estimators: the number of decision trees you're going to build.\n",
    "# higher the better, but of course the higher it is the slower the model is going to be.\n",
    "num_dtrees_list = [5, 10, 20]\n",
    "\n",
    "# max_depth: maximum depth of decision trees\n",
    "depth_list = [3,5,8]\n",
    "\n",
    "# criterion: gini or entropy\n",
    "criterion_list = [\"gini\", \"entropy\"]\n",
    "\n",
    "# min_samples_split: the minimum number of samples required to split an internal node\n",
    "min_split_list = [2,3,5]\n",
    "\n",
    "# min_samples_leaf: the minimum number of samples required to be at a leaf node:\n",
    "min_leaf_list = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: ('auto', 5, 3, 'gini')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 5, 3, 'entropy')\n",
      "Accuracy: 0.17199024687595246\n",
      "Precision: 0.06488479262672811\n",
      "Recall: 0.1308550185873606\n",
      "F1: 0.08675292667898953\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 5, 5, 'gini')\n",
      "Accuracy: 0.17177689728741236\n",
      "Precision: 0.0656221198156682\n",
      "Recall: 0.12875226039783\n",
      "F1: 0.08693528693528693\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 5, 5, 'entropy')\n",
      "Accuracy: 0.17183785431270954\n",
      "Precision: 0.06617511520737326\n",
      "Recall: 0.12890484739676838\n",
      "F1: 0.0874543239951279\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 5, 8, 'gini')\n",
      "Accuracy: 0.1713806766229808\n",
      "Precision: 0.07023041474654378\n",
      "Recall: 0.12370129870129871\n",
      "F1: 0.08959435626102294\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 5, 8, 'entropy')\n",
      "Accuracy: 0.17141115513562938\n",
      "Precision: 0.0680184331797235\n",
      "Recall: 0.1248730964467005\n",
      "F1: 0.0880668257756563\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 10, 3, 'gini')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 10, 3, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 10, 5, 'gini')\n",
      "Accuracy: 0.17199024687595246\n",
      "Precision: 0.06506912442396313\n",
      "Recall: 0.13074074074074074\n",
      "F1: 0.08689230769230769\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 10, 5, 'entropy')\n",
      "Accuracy: 0.17195976836330387\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.13038674033149172\n",
      "F1: 0.08697788697788697\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 10, 8, 'gini')\n",
      "Accuracy: 0.17077110637000914\n",
      "Precision: 0.0704147465437788\n",
      "Recall: 0.11974921630094044\n",
      "F1: 0.08868253047011028\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 10, 8, 'entropy')\n",
      "Accuracy: 0.17128924108503504\n",
      "Precision: 0.06728110599078341\n",
      "Recall: 0.1243611584327087\n",
      "F1: 0.08732057416267944\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 20, 3, 'gini')\n",
      "Accuracy: 0.17177689728741236\n",
      "Precision: 0.06764976958525346\n",
      "Recall: 0.12765217391304348\n",
      "F1: 0.08843373493975905\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 20, 3, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 20, 5, 'gini')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 20, 5, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 20, 8, 'gini')\n",
      "Accuracy: 0.17147211216092656\n",
      "Precision: 0.06894009216589861\n",
      "Recall: 0.12487479131886477\n",
      "F1: 0.08883610451306413\n",
      "————————————————————————————————\n",
      "Model parameters: ('auto', 20, 8, 'entropy')\n",
      "Accuracy: 0.17131971959768363\n",
      "Precision: 0.06820276497695853\n",
      "Recall: 0.12416107382550337\n",
      "F1: 0.08804283164782867\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 5, 3, 'gini')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 5, 3, 'entropy')\n",
      "Accuracy: 0.17180737580006095\n",
      "Precision: 0.06672811059907834\n",
      "Recall: 0.12836879432624113\n",
      "F1: 0.08781079442086112\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 5, 5, 'gini')\n",
      "Accuracy: 0.1719292898506553\n",
      "Precision: 0.06599078341013825\n",
      "Recall: 0.12971014492753624\n",
      "F1: 0.08747709224190592\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 5, 5, 'entropy')\n",
      "Accuracy: 0.17205120390124962\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.13111111111111112\n",
      "F1: 0.08713846153846154\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 5, 8, 'gini')\n",
      "Accuracy: 0.17110637000914355\n",
      "Precision: 0.06912442396313365\n",
      "Recall: 0.12234910277324633\n",
      "F1: 0.08833922261484098\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 5, 8, 'entropy')\n",
      "Accuracy: 0.1710149344711978\n",
      "Precision: 0.07059907834101382\n",
      "Recall: 0.12120253164556963\n",
      "F1: 0.08922539312754804\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 10, 3, 'gini')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 10, 3, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 10, 5, 'gini')\n",
      "Accuracy: 0.17189881133800672\n",
      "Precision: 0.06599078341013825\n",
      "Recall: 0.1294755877034358\n",
      "F1: 0.08742368742368743\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 10, 5, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 10, 8, 'gini')\n",
      "Accuracy: 0.17110637000914355\n",
      "Precision: 0.06894009216589861\n",
      "Recall: 0.12242225859247136\n",
      "F1: 0.0882075471698113\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 10, 8, 'entropy')\n",
      "Accuracy: 0.17165498323681805\n",
      "Precision: 0.06654377880184333\n",
      "Recall: 0.127336860670194\n",
      "F1: 0.087409200968523\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 20, 3, 'gini')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 20, 3, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 20, 5, 'gini')\n",
      "Accuracy: 0.1719292898506553\n",
      "Precision: 0.0656221198156682\n",
      "Recall: 0.12992700729927006\n",
      "F1: 0.08720146968769135\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 20, 5, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 20, 8, 'gini')\n",
      "Accuracy: 0.1713501981103322\n",
      "Precision: 0.0696774193548387\n",
      "Recall: 0.12373158756137478\n",
      "F1: 0.08915094339622641\n",
      "————————————————————————————————\n",
      "Model parameters: ('sqrt', 20, 8, 'entropy')\n",
      "Accuracy: 0.1715635476988723\n",
      "Precision: 0.06912442396313365\n",
      "Recall: 0.1254180602006689\n",
      "F1: 0.08912655971479501\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 5, 3, 'gini')\n",
      "Accuracy: 0.17195976836330387\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.13038674033149172\n",
      "F1: 0.08697788697788697\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 5, 3, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 5, 5, 'gini')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 5, 5, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06543778801843318\n",
      "Recall: 0.13075506445672191\n",
      "F1: 0.08722358722358722\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 5, 8, 'gini')\n",
      "Accuracy: 0.17110637000914355\n",
      "Precision: 0.06912442396313365\n",
      "Recall: 0.12234910277324633\n",
      "F1: 0.08833922261484098\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 5, 8, 'entropy')\n",
      "Accuracy: 0.17131971959768363\n",
      "Precision: 0.0696774193548387\n",
      "Recall: 0.12352941176470589\n",
      "F1: 0.08909840895698291\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 10, 3, 'gini')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 10, 3, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 10, 5, 'gini')\n",
      "Accuracy: 0.17195976836330387\n",
      "Precision: 0.06506912442396313\n",
      "Recall: 0.13049907578558226\n",
      "F1: 0.08683886838868389\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 10, 5, 'entropy')\n",
      "Accuracy: 0.17199024687595246\n",
      "Precision: 0.06506912442396313\n",
      "Recall: 0.13074074074074074\n",
      "F1: 0.08689230769230769\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 10, 8, 'gini')\n",
      "Accuracy: 0.17128924108503504\n",
      "Precision: 0.07078341013824885\n",
      "Recall: 0.12287999999999999\n",
      "F1: 0.08982456140350877\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 10, 8, 'entropy')\n",
      "Accuracy: 0.17131971959768363\n",
      "Precision: 0.07078341013824885\n",
      "Recall: 0.12307692307692308\n",
      "F1: 0.08987712112346401\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 20, 3, 'gini')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 20, 3, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 20, 5, 'gini')\n",
      "Accuracy: 0.17189881133800672\n",
      "Precision: 0.0656221198156682\n",
      "Recall: 0.1296903460837887\n",
      "F1: 0.08714810281517747\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 20, 5, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 20, 8, 'gini')\n",
      "Accuracy: 0.17131971959768363\n",
      "Precision: 0.0696774193548387\n",
      "Recall: 0.12352941176470589\n",
      "F1: 0.08909840895698291\n",
      "————————————————————————————————\n",
      "Model parameters: ('log2', 20, 8, 'entropy')\n",
      "Accuracy: 0.17131971959768363\n",
      "Precision: 0.0680184331797235\n",
      "Recall: 0.12424242424242424\n",
      "F1: 0.08790946992257295\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 5, 3, 'gini')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 5, 3, 'entropy')\n",
      "Accuracy: 0.17074062785736058\n",
      "Precision: 0.044976958525345626\n",
      "Recall: 0.13443526170798897\n",
      "F1: 0.06740331491712707\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 5, 5, 'gini')\n",
      "Accuracy: 0.17049679975617188\n",
      "Precision: 0.04976958525345622\n",
      "Recall: 0.12765957446808512\n",
      "F1: 0.0716180371352785\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 5, 5, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 5, 8, 'gini')\n",
      "Accuracy: 0.170710149344712\n",
      "Precision: 0.06764976958525346\n",
      "Recall: 0.12032786885245901\n",
      "F1: 0.08660766961651918\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 5, 8, 'entropy')\n",
      "Accuracy: 0.1713501981103322\n",
      "Precision: 0.06912442396313365\n",
      "Recall: 0.12396694214876033\n",
      "F1: 0.08875739644970414\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 10, 3, 'gini')\n",
      "Accuracy: 0.17159402621152087\n",
      "Precision: 0.06543778801843318\n",
      "Recall: 0.12746858168761221\n",
      "F1: 0.08647990255785627\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 10, 3, 'entropy')\n",
      "Accuracy: 0.1711978055470893\n",
      "Precision: 0.05105990783410138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.13381642512077294\n",
      "F1: 0.07391594396264176\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 10, 5, 'gini')\n",
      "Accuracy: 0.17195976836330387\n",
      "Precision: 0.06506912442396313\n",
      "Recall: 0.13049907578558226\n",
      "F1: 0.08683886838868389\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 10, 5, 'entropy')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 10, 8, 'gini')\n",
      "Accuracy: 0.1710149344711978\n",
      "Precision: 0.0687557603686636\n",
      "Recall: 0.12189542483660132\n",
      "F1: 0.08791985857395404\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 10, 8, 'entropy')\n",
      "Accuracy: 0.1715635476988723\n",
      "Precision: 0.06728110599078341\n",
      "Recall: 0.12629757785467127\n",
      "F1: 0.08779314491882141\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 20, 3, 'gini')\n",
      "Accuracy: 0.17202072538860103\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.1308687615526802\n",
      "F1: 0.08708487084870849\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 20, 3, 'entropy')\n",
      "Accuracy: 0.17199024687595246\n",
      "Precision: 0.06525345622119816\n",
      "Recall: 0.13062730627306274\n",
      "F1: 0.08703134603564842\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 20, 5, 'gini')\n",
      "Accuracy: 0.17180737580006095\n",
      "Precision: 0.06672811059907834\n",
      "Recall: 0.12836879432624113\n",
      "F1: 0.08781079442086112\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 20, 5, 'entropy')\n",
      "Accuracy: 0.17195976836330387\n",
      "Precision: 0.06451612903225806\n",
      "Recall: 0.1308411214953271\n",
      "F1: 0.08641975308641975\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 20, 8, 'gini')\n",
      "Accuracy: 0.17144163364827797\n",
      "Precision: 0.06728110599078341\n",
      "Recall: 0.1254295532646048\n",
      "F1: 0.08758248350329934\n",
      "————————————————————————————————\n",
      "Model parameters: (0.2, 20, 8, 'entropy')\n",
      "Accuracy: 0.17183785431270954\n",
      "Precision: 0.06580645161290323\n",
      "Recall: 0.1291139240506329\n",
      "F1: 0.08717948717948718\n",
      "————————————————————————————————\n"
     ]
    }
   ],
   "source": [
    "resultsDF = grid_cv_RF(X_train, Y_train, max_feats, num_dtrees_list, depth_list, criterion_list, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(resultsDF).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(log2, 10, 8, entropy)</th>\n",
       "      <td>0.171320</td>\n",
       "      <td>0.089877</td>\n",
       "      <td>0.070783</td>\n",
       "      <td>0.123077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(log2, 10, 8, gini)</th>\n",
       "      <td>0.171289</td>\n",
       "      <td>0.089825</td>\n",
       "      <td>0.070783</td>\n",
       "      <td>0.122880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(auto, 5, 8, gini)</th>\n",
       "      <td>0.171381</td>\n",
       "      <td>0.089594</td>\n",
       "      <td>0.070230</td>\n",
       "      <td>0.123701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(sqrt, 5, 8, entropy)</th>\n",
       "      <td>0.171015</td>\n",
       "      <td>0.089225</td>\n",
       "      <td>0.070599</td>\n",
       "      <td>0.121203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(sqrt, 20, 8, gini)</th>\n",
       "      <td>0.171350</td>\n",
       "      <td>0.089151</td>\n",
       "      <td>0.069677</td>\n",
       "      <td>0.123732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy        F1  Precision    Recall\n",
       "(log2, 10, 8, entropy)  0.171320  0.089877   0.070783  0.123077\n",
       "(log2, 10, 8, gini)     0.171289  0.089825   0.070783  0.122880\n",
       "(auto, 5, 8, gini)      0.171381  0.089594   0.070230  0.123701\n",
       "(sqrt, 5, 8, entropy)   0.171015  0.089225   0.070599  0.121203\n",
       "(sqrt, 20, 8, gini)     0.171350  0.089151   0.069677  0.123732"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(\"F1\",ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model uses log2, 10, 8, entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Cross Validation\n",
    "Writing 8x nested for-loops to search over all parameters can be a bit cumbersome. Luckily, sklearn has a very nice [GridSearchCV class](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) that can do all this grid searching and cross validating for us!\n",
    "\n",
    "GridSearchCV requires:\n",
    "* an estimator object(IE: LogisticRegression(), SVC(), KNeighborsClassifier(), etc)\n",
    "* param_grid: a dictionary mapping parameter names to lists of values to search over. We'll see an example below\n",
    "* scoring: a string. See the valid inputs here: http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "* n_jobs: number of jobs to run in parallel to speed up the computation. GridSearchCV can take a long time if you have a lot of different parameter combinations to try out. So it's important to set this parameter if your laptop/desktop has a lot of cores that you want to take advantage of.\n",
    "* cv: int, number of cross validation folds to use\n",
    "* refit: boolean, if true, at the end of the Grid Search, GridSearchCV will refit the entire data with a model using the optimal parameters(you can then access this with the GridSearchCV.best\\_parameters_ attribute).\n",
    "\n",
    "Here's an example of how we'd use it for some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>{u'penalty': u'l1', u'C': 0.01}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.127041</td>\n",
       "      <td>0.143394</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>{u'penalty': u'l2', u'C': 0.01}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>0.07565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151261</td>\n",
       "      <td>0.186551</td>\n",
       "      <td>0.121739</td>\n",
       "      <td>0.138702</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.184486</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.034832</td>\n",
       "      <td>0.040748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{u'penalty': u'l1', u'C': 0.1}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C  \\\n",
       "0       0.001010         0.001104         0.000000          0.000000    0.01   \n",
       "1       0.001523         0.000942         0.127041          0.143394    0.01   \n",
       "2       0.000837         0.000853         0.000000          0.000000     0.1   \n",
       "\n",
       "  param_penalty                           params  rank_test_score  \\\n",
       "0            l1  {u'penalty': u'l1', u'C': 0.01}                9   \n",
       "1            l2  {u'penalty': u'l2', u'C': 0.01}                8   \n",
       "2            l1   {u'penalty': u'l1', u'C': 0.1}                9   \n",
       "\n",
       "   split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "0           0.000000             0.00000       ...                  0.000000   \n",
       "1           0.092593             0.07565       ...                  0.151261   \n",
       "2           0.000000             0.00000       ...                  0.000000   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "0            0.000000           0.000000            0.000000   \n",
       "1            0.186551           0.121739            0.138702   \n",
       "2            0.000000           0.000000            0.000000   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.000000            0.000000      0.000060        0.000048   \n",
       "1           0.180328            0.184486      0.000103        0.000061   \n",
       "2           0.000000            0.000000      0.000037        0.000021   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.000000         0.000000  \n",
       "1        0.034832         0.040748  \n",
       "2        0.000000         0.000000  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of how to use GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def sample_cross_val_param_search():\n",
    "    # parameters to search over\n",
    "    penalties = ['l1', 'l2']\n",
    "    c_vals = [10**i for i in range(-2, 3)]\n",
    "    param_grid = {'penalty': penalties, 'C': c_vals}\n",
    "    \n",
    "    # make some fake data\n",
    "    n_samples = 1000\n",
    "    feats = 10\n",
    "    x_train = np.random.random((n_samples, feats))\n",
    "    y_train = (np.random.random(n_samples) > 0.5) # randomly generate True/False labels\n",
    "    \n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, 'f1', cv=5)\n",
    "    grid.fit(x_train, y_train)\n",
    "    return grid\n",
    "grid = sample_cross_val_param_search()\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "print(\"GridSearchCV results:\")\n",
    "results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Best Parameters: {'penalty': 'l1', 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print('Best estimator: {}'.format(grid.best_estimator_))\n",
    "print('Best Parameters: {}'.format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Use GridSearchCV to do the same parameter search you did above. What is the best performing set of parameters? Do they match up with what you had above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) As mentioned earlier, GridSearchCV.best\\_estimator\\_ will contain a model with parameters set to the best performing set of parameters fitted on the entire training set. Use this best estimator on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Random Forests has a [feature importances](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_) attribute. Which features are the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Rerun the classifier with a different scoring metric. Do the feature importances change? Which features are most important now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://etav.github.io/projects/spam_message_classifier_naive_bayes.html\n",
    "* http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
